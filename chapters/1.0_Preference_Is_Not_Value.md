# 章一・零 (Chapter 1.0 / 6.0)

## Preference Is Not Value

**Progress: 33.3%**

---

「価値は選択ではない。残存条件である。」  
*Value is not a choice. It is a condition for remaining.*

---

### 1.0.1 Why This Distinction Matters

If preference and value are not separated cleanly, normativity collapses.

Most theories fail here.

They treat value as:

- a strong preference,
- a stable desire,
- a reinforced bias.

This mistake makes obligation impossible.

If value is only preference, then any "ought" can be revised away.

**That is not how normativity behaves.**

---

### 1.0.2 What Preference Actually Is

From Book I, preference was defined precisely:

> Preference is an internally generated asymmetry that biases future admissibility.

Preference:

- can persist,
- can shape action,
- can guide agency.

**But preference remains revisable.**

A system may abandon a preference and remain the same system.

This is the critical weakness.

---

### 1.0.3 The Revisability Test

We now introduce a diagnostic test:

> **If a system can abandon X and still remain evaluatively coherent, then X is not a value.**

Apply this test universally.

Preferences pass through it easily.

Values do not.

**Revisability is the dividing line.**

---

### 1.0.4 Why Strength Does Not Help

One might argue:

> "Values are just very strong preferences."

This fails.

Strength does not produce obligation.  
Persistence does not produce bindingness.

A strong preference can still be abandoned.  
A value cannot—without loss of identity.

**Magnitude is irrelevant.**

---

### 1.0.5 Preference Can Conflict Without Crisis

Preferences conflict all the time.

A system may:

- prefer efficiency,
- prefer novelty,
- prefer stability—

and revise the balance among them without contradiction.

Such revision may change behavior, but not selfhood.

**Value conflict is different.**

When values conflict, something must break.

---

### 1.0.6 Why Learning Does Not Create Value

Learning updates preferences.

Reinforcement learning, Bayesian updating, adaptation—all of these revise bias structures.

But learning alone cannot create value.

Why?

Because learning presupposes:

- update rules,
- criteria for revision,
- a meta-stability of identity.

Learning changes what is preferred, not what **must not be changed**.

---

### 1.0.7 The Identity Criterion

We can now state the identity criterion formally:

> **Values are constraints that define the identity of an evaluative system across preference revision.**

If removing X dissolves the system's evaluative coherence, then X is a value.

If not, it is preference.

---

### 1.0.8 Why Value Is Scarce

Most systems have no values.

They have:

- many preferences,
- many biases,
- many learned tendencies.

But no inviolable constraints.

Value is rare because it is costly.

It limits revision.  
It restricts flexibility.  
It imposes fragility.

**Systems that maximize adaptability tend to avoid values.**

---

### 1.0.9 The First Normative Asymmetry

Preference introduces asymmetry in outcomes.  
Value introduces asymmetry in **revision itself**.

This is the first normative asymmetry.

Not "better outcome,"  
but "forbidden change."

**This is the birth of normativity.**

---

### 1.0.10 What Has Been Excluded

We can now exclude firmly:

- Value ≠ desire
- Value ≠ reward
- Value ≠ utility
- Value ≠ learned bias

Any theory that equates these has already failed.

---

### 1.0.11 Transition Marker

We now understand what value is not.

But a problem remains.

If value is an inviolable constraint, how does such a constraint arise at all?

Why doesn't every system simply revise everything?

This leads directly to the next chapter.

---

「理由なき好みは、規範にならない。」  
*Preference without grounding cannot bind.*

---

[← Previous: Chapter 0.2](0.2_Methodological_Constraints.md) | [Next: Chapter 1.1 →](1.1_The_Problem_of_Arbitrary_Preference.md)
